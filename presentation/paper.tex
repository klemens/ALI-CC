\documentclass[a4paper,12pt,titlepage=false]{scrreprt}

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, left=2cm, right=2cm, top=1.5cm, bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{stmaryrd}
\usepackage{listings}
\usepackage{amsmath}
\usepackage[hidelinks]{hyperref}

\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\tikzstyle{block} = [rectangle, draw, text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']

\definecolor{light-gray}{gray}{0.6}

\titlehead{Seminar: Anwendungen Linguistische Informatik \hfill Vortrag am 06.07.2015}
\title{\vspace{3cm}Common Crawl}
\subtitle{Analyse des Crawling-Prozesses und der Textextraktion}
\author{Klemens Schölhorn \and Erik Körner \and Kai Hainke}

% show toc on titlepage
\setuptoc{toc}{leveldown}

% no page number on titlepage
\renewcommand*{\titlepagestyle}{empty}

\begin{document}

\maketitle
\vspace{2cm}
\tableofcontents

\onehalfspacing

\chapter{Common Crawl}

\section{Organisation}

\begin{itemize}
    \item Non-Profit Organisation
    \item 2007 durch Gil Elbaz gegründet
    \item Mitglieder stellen unentgeldlich Fachwissen und Zeit zur Verfügung
\end{itemize}
\textbf{``Our goal is to democratize the data so everyone, not just big companies, can do high quality research and analysis.''}
\hfill-- \textit{Common Crawl Foundation}

\begin{itemize}
    \item (Fast) monatliche Crawls des Web und freie Bereitstellung der Datensätze
    \item Letzter Crawl: 168 TB, 2,1 Mrd. Websites\footnote{\url{http://blog.commoncrawl.org/2015/05/april-2015-crawl-archive-available/} (Stand: Mai 2015)}
    \item Viele Code-Beispiele zur Verarbeitung der Daten
    \item Hosting gesponsert von AWS (Public Data Set),\\gesamt: 541 TB\footnote{\url{https://aws.amazon.com/datasets/41740}}
\end{itemize}


\section{Technik}

\begin{itemize}
    \item CCBot: modifizierter Apache Nutch 1.7
        \begin{itemize}
            \item Basiert auf Lucene, Solr und Hadoop
            \item Verteiltes Crawling bei AWS
            \item „Freundlicher Crawler“: beachtet robots.txt und erzeugt so wenig Last wie möglich
        \end{itemize}
    \item Speicherung im WARC-Format (Web ARChive)
    \item Drei verschiedene Ergebnisse
        \begin{itemize}
            \item WARC: Enthält komplette HTTP Konversationen getrennt nach
                Anfrage und Antwort (Records)
            \item WAT: Metadaten für jeden Record
            \item WET: Reintext-Extrakt der Antworten
        \end{itemize}
\end{itemize}


\chapter{Analyse}

Die Analyse erfolgte anhand des Crawls vom April 2015, des zum Zeitpunkt des
Vortrages aktuellsten Crawls. Der komplette Crawl besitzt eine Größe von über
168 TB\footnote{\url{http://blog.commoncrawl.org/2015/05/april-2015-crawl-archive-available/}}.
Da eine Bearbeitung dieser Datenmenge auf eigenen Servern aufgrund von
Bandbreitenbeschränkungen und eine Bearbeitung in AWS mit Hilfe von EC2-Instanzen
aufgrund von finanziellen Einschränkungen nicht möglich war, haben wir unsere
Analyse auf die WAT-Dateien beschränkt, die aus knapp 39000 Dateien zu
durchschnittlich je 315 MiB bestehen und damit zusammen nur rund 12 TB groß sind.
Davon konnten wir bis zum Vortrag ca. 53 \% analysieren.

\section{Übersicht}

\begin{wrapfigure}{r}{.54\textwidth}
\label{wrap-fig:1}
\begin{tikzpicture}[auto]
    \node[block] (CC) at (0,0) {Common Crawl};
    \node[block, right=3.5cm of CC] (META) {meta-extractor};
    \node[block, below=1.5cm of META] (MONET) {MonetDB};
    \node[block, left=3.5cm of MONET] (R) {R};
    \path[line] (CC) -- node {Download (curl)} (META);
    \path[line] (META) -- node {csv} (MONET);
    \path[line] (MONET) -- node {MonetDB.R} (R);
\end{tikzpicture}
\end{wrapfigure}

Die nebenstehende Abbildung zeigt den strukturellen Ablauf der Analyse.

Der Download und die Extraktion der benötigten Daten mit Hilfe des
\texttt{meta-extractor}s erfolgte parallel auf dem zur Verfügung gestellten
Praktikumsrechner und auf einem privaten Server eines Autors. Dabei wurden keine
Dateien lokal zwischengespeichert, sondern die WAT-Dateien mittels Unix-Pipes
direkt von \texttt{curl} in den \texttt{meta-extractor} und anschließend die
resultierenden csv-Dateien mittels \texttt{ssh} direkt auf den genannten
privaten Server zur weiteren Analyse geladen.

Dazu wurden die Daten in die spaltenorientierte Datenbank
\texttt{MonetDB}\footnote{\url{https://www.monetdb.org/}} importiert, um
anschließend mittels der \texttt{R}-Anbindung darauf in \texttt{R} Analysen
durchführen zu können.

\section{\texttt{meta-extractor}}

Der \texttt{meta-extractor} ist ein in \texttt{C++} geschriebenes Programm,
das sequentiell Metadaten-WARC-Records aus WAT-Dateien auslesen und daraus
die im Abschnitt \ref{csv-data} beschriebenen Metadaten als csv-Datei extrahieren
kann.

Dazu wurden neben einem csv-Writer ein standardkonformer WARC-Parser und eine
Trie-Implementierung für den effizienten Lookup des \texttt{Public Suffix} einer
Domain aus der \texttt{Public Suffix}-Liste\footnote{\url{https://publicsuffix.org/}} implementiert. Außerdem wurde auf RapidJSON\footnote{\url{https://github.com/miloyip/rapidjson}}
zum schnellen Parsen der WAT-Metadaten, auf die URI-Klasse des
POCO-Projektes\footnote{\url{http://pocoproject.org/}} und auf
TCLAP\footnote{\url{http://tclap.sourceforge.net/}} für das CLI zurückgegriffen. Das Test-Framework Catch\footnote{\url{https://github.com/philsquared/Catch}} wurde für Unit-Tests der selbst geschriebenen Komponenten und für Integrationstests verwendet.

Das beiliegende \texttt{Makefile} ist für den GCC C++-Compiler konzipiert, der Code lässt sich jedoch mit jedem standardkonformen C++11-Compiler übersetzen.

\section{Hilfsmittel}

Neben dem \texttt{meta-extractor} wurden noch einige Skripte u.\,a. zur
Steuerung des Downloads und der Extraktion, dem Import in \texttt{MonetDB} und
der Analyse in \texttt{R} geschrieben. Das erstgenannte ist etwas umfangreicher
und daher neben dem Hauptprogramm auch Teil des
Projektrepositories\footnote{\url{https://github.com/klemens/ALI-CC/}}.

\section{Gesammelte Metadaten}
\label{csv-data}

Die in folgender Tabelle beschriebenen Metadaten wurden vom \texttt{meta-extractor}
extrahiert und für die weiteren Analysen verwendet:

\begin{center}
\begin{tabular}{lcl}
    \textbf{Name}  & \textbf{Datentyp} & \textbf{Beispiel} \\ \hline\hline
    UUID           & uint128           & \textcolor{light-gray}{14a939f4-2355-11e5-b5f7-727283247c7f} \\ \hline
    Zeitstempel    & string            & \textcolor{light-gray}{2015-07-06T11:03:18T} \\ \hline
    Verwendung TLS & bool              & https ja/nein \\ \hline
    Hostname       & string            & \textcolor{light-gray}{amazon.co.uk} \\ \hline
    TLD            & string            & \textcolor{light-gray}{uk} \\ \hline
    Public Suffix  & string            & \textcolor{light-gray}{co.uk} \\ \hline
    Pfadtiefe      & uint8             & \textit{test/cool/cool.html} $\rightarrow$ \textcolor{light-gray}{3} \\ \hline
    Pfadlänge      & uint16            & \textit{test/cool/cool.html} $\rightarrow$ \textcolor{light-gray}{19} \\ \hline
    Server         & string            & \textcolor{light-gray}{apache}, \textcolor{light-gray}{nginx}, \dots \\ \hline
    Kompression    & bool              & \textcolor{light-gray}{gzip}, \textcolor{light-gray}{deflate} in \textit{Content-Encoding} \\ \hline
    Cookies        & bool              & \textit{Set-Cookie} vorhanden \\ \hline
    MIME-Typ       & string            & \textcolor{light-gray}{text/html}, \textcolor{light-gray}{application/xml}, \dots\\ \hline
    Charset        & string            & \textcolor{light-gray}{utf}, \textcolor{light-gray}{iso-8859}, \dots \\ \hline
    Verwendung CDN & bool              & CDN ja/nein \\ \hline
    Interne Links  & uint16            & Anzahl relative Links + gleicher Host \\ \hline
    Externe Links  & uint16            & Anzahl ausgehende Links
\end{tabular}
\end{center}


\chapter{Ergebnisse}


\section{TLD}

\begin{center}
    \includegraphics[width=0.9\textwidth]{plots/plot_tld_top10}
\end{center}

\noindent
Die TLD .com, die vor allem von kommerziellen Unternehmen genutzt wird, ist mit über 70\% mit Abstand am meisten vertreten. Danach folgt die eher von nicht-kommerziellen Organisationen benutzte .org.

\begin{wrapfigure}{r}{.63\textwidth}
    \label{wrap-fig:2}
    \includegraphics[width=.63\textwidth]{plots/plot_tld_zipf}
\end{wrapfigure}

Mit .edu und .gov befinden sich zwei TLDs unter den Top-5, die primär von US-amerikanischen Bildungs- und Regierungsorganisationen verwendet werden. Anschließend folgen länderspezifische TLDs, wobei englischsprachige und europäische Länder den überwiegenden Anteil stellen.

Grundsätzlich scheinen die TLDs näherungsweise zipfverteilt zu sein.

% prevent wrapfigure from floating into next pages
\clearpage

\begin{wrapfigure}{r}{.45\textwidth}
    \label{wrap-fig:3}
    \def\svgwidth{.45\textwidth}
    \input{images/selection_bias_clean.pdf_tex}
\end{wrapfigure}

Die vom Common Crawl erfassten Seiten stellen eine nicht randomisierte Stichprobe aus der Gesamtheit aller Websites dar. Daher unterliegen die Ergebnisse einer statistischen Verzerrung im Vergleich zur wahren Verteilung der Grundgesamtheit aller Websites, dem Selection Bias. Um abzuschätzen, in wie weit die Auswahl von Websites durch den Common Crawl Prozess im Vergleich zu einer randomisierten Auswahl verzerrt ist, vergleichen wir die Verteilung der TLDs mit Daten von Google. Auch diese Daten unterliegen natürlich dem Selection Bias, haben aber eine größere Stichprobengröße.

Da Google selbst keine offiziellen Statistiken über die erfassten TLD bereitstellt, wurden die Daten über eine speziell formulierte Suchanfrage der Form \texttt{site:.com} erhoben. Dies liefert alle Seiten, die in Ihrer Domain den String .com enthalten, was näherungsweise alle Seiten der TLD .com sind. Auf diese Weise wurden die Trefferzahlen der 61 häufigsten Domains ermittelt und in Relation zueinander gesetzt. Aufgrund der nicht vollständigen Abdeckung aller TLDs sind die relativen Anteile geringfügig zu hoch.

\begin{center}
    \includegraphics[width=.9\textwidth]{plots/plot_tld_comparison}
\end{center}

Es zeigt sich, dass mit Ausnahme englischsprachiger Länder (besonders asiatische) länderspezifische TLDs stark unterrepräsentiert sind (z.\,B. Südkorea .kr), während amerikanische TLDs (.edu, .gov) und die kommerzielle TLD .com stark überrepräsentiert sind. Typische TLDs von Websites für Medienstreaming (z.\,B. .tv und .fm) sind ebenfalls unterrepräsentiert.

\section{Public Suffix}

\begin{center}
    \includegraphics[width=.9\textwidth]{plots/plot_pub_suffixes_top50}
\end{center}

\noindent
Wie zu erwarten zeigt sich bei den Public Suffixes ein ähnliches Bild wie bei den TLDs. Ein extremer Ausreißer ist der Blog-Dienst blogger.com mit seinen auf blogspot.com gehosteten Blogs.

Außerdem erscheinen nun z.\,B. .co.uk, .org.uk, etc. weit vor .uk (nicht im Graph), da die Registrierung von Domains direkt unter .uk erst seit kurzem erlaubt und damit noch nicht weit verbreitet ist.

\section{Mime-Type}

\begin{wrapfigure}{r}{.6\textwidth}
    \includegraphics[width=.6\textwidth]{plots/plot_mime}
\end{wrapfigure}

Es werden fast ausschließlich HTML-Textdaten gecrawlt, was den Erwartungen entspricht. Andere typische gecrawlte Typen sind Bilder, PDFs, Feeds und Kalender.

Da die eigentlichen Daten nicht analysiert wurden und die HTTP-Header möglicherweise nicht immer korrekt sind (z.\,B. falsch konfigurierter Webserver), sollten diese Daten kritisch gesehen werden.


% prevent wrapfigure from floating into next pages
\clearpage

\section{TLS}
\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_https}}
Der überwiegende Anteil an Websites, welche gecrawlt werden, sind unverschlüsselt.



\section{Encoding}
\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_charset}}

Das Encoding gibt erste Hinweise darauf, welche Zeichen auf den Websites verwendet werden. Die Mehrheit an gecrawlten Websites benutzt ein utf- oder iso-8859 Encoding. TODO

\section{Server}
\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_server}}


\chapter{Textextraktion}

\centering{
    \includegraphics[trim=0 105px 0 0, clip=true, width=.9\textwidth]{images/guardian-website}
}

\scriptsize
\vspace{.1cm}
\url{http://www.theguardian.com/world/2013/jun/09/edward-snowden-nsa-whistleblower-surveillance}

\scriptsize
Edward Snowden: the whistleblower behind the NSA surveillance revelations $|$ US news $|$ The Guardian

\section{jWarcEx}

\begin{lstlisting}[breaklines=true]
Skip to main content

browse all sections close

Glenn Greenwald on security and liberty

Edward Snowden: the whistleblower behind the NSA surveillance revelations

The 29-year-old source behind the biggest intelligence leak in the NSA's history explains his motives, his uncertain future and why he never intended on hiding in the shadows

Q\&A with NSA whistleblower Edward Snowden: ``I do not expect to see home again''

Glenn Greenwald, Ewen MacAskill and Laura Poitras in Hong Kong

Tuesday 11 June 2013 09.00 EDT Last modified on Saturday 4 October 2014 10.54 EDT

Sorry, your browser is unable to play this video.

The individual responsible for one of the most significant leaks in US political history is Edward Snowden, a 29-year-old former technical assistant for the CIA and current employee of the defence contractor Booz Allen Hamilton. Snowden has been working at the National Security Agency for the last four years as an employee of various outside contractors, including Booz Allen and Dell.

The Guardian, after several days of interviews, is revealing his identity at his request. From the moment he decided to disclose numerous top-secret documents to the public, he was determined not to opt for the protection of anonymity. ``I have no intention of hiding who I am because I know I have done nothing wrong,'' he said.

(...)
\end{lstlisting}

\section{Mozilla Readability.js}

\begin{lstlisting}[breaklines=true]
Edward Snowden: the whistleblower behind the NSA surveillance revelations

Laura Poitras

The individual responsible for one of the most significant leaks in US political history is Edward Snowden, a 29-year-old former technical assistant for the CIA and current employee of the defence contractor Booz Allen Hamilton. Snowden has been working at the National Security Agency for the last four years as an employee of various outside contractors, including Booz Allen and Dell.

The Guardian, after several days of interviews, is revealing his identity at his request. From the moment he decided to disclose numerous top-secret documents to the public, he was determined not to opt for the protection of anonymity. ``I have no intention of hiding who I am because I know I have done nothing wrong,'' he said.

(...)
\end{lstlisting}

\section{Common Crawl}

\begin{lstlisting}[breaklines=true]
Lance Armstrong, Tour de France champ, retires | abc7.com

GO

Personalize your weather by entering a location.

Sorry, but the location you entered was not found. Please try again.

Sections

Traffic

Video

Los AngelesOrange CountyInland EmpireVentura CountyCalifornia

Home

Accuweather

Traffic

Video

Photos

Mobile Apps

Local News Los AngelesOrange CountyInland EmpireVentura CountyCalifornia

Map My News

Shows Live Well Network

Follow Us

BREAKING NEWS

(...)
\end{lstlisting}

\vspace{.2cm}
\url{http://abc7.com/archive/7962461}

\section{jWarcEx}

\begin{lstlisting}[breaklines=true]
Personalize your weather by entering a location.

Sorry, but the location you entered was not found. Please try again.

\vspace{.2cm}
Sections Traffic Video Los AngelesOrange CountyInland EmpireVentura CountyCalifornia

Home Accuweather Traffic Video Photos Mobile Apps

Los AngelesOrange CountyInland EmpireVentura CountyCalifornia

\quad BREAKING NEWS ABC shows live and on-demand -- Download the WATCH ABC app!

\vspace{.2cm}
Lance Armstrong, Tour de France champ, retires

Seven-time Tour de France champion Lance Armstrong is retiring. He ends his career amidst a federal doping investigation.

February 16, 2011 12:00:00 AM PST

Seven-time Tour de France champion Lance Armstrong said he's retiring from the professional cycling circuit, this time for good. The 39-year-old is calling it ``Retirement 2.0'' and is making it clear there is no reset button this time.

\vspace{.2cm}
He says he wants to spend more time with his children and dedicate himself even more to the fight against cancer with his "Livestrong" Foundation.

(...)
\end{lstlisting}

\vspace{.2cm}
\url{http://abc7.com/archive/7962461}


\end{document}
