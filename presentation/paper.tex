\documentclass[a4paper,12pt,titlepage=false]{scrreprt}

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, left=2cm, right=2cm, top=1.5cm, bottom=2cm]{geometry}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{tikz}
\usepackage{stmaryrd}
\usepackage{listings}
\usepackage{amsmath}
\usepackage[hidelinks]{hyperref}

\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\tikzstyle{block} = [rectangle, draw, text width=7em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']

\definecolor{light-gray}{gray}{0.6}

\titlehead{Seminar: Anwendungen Linguistische Informatik \hfill Vortrag am 06.07.2015}
\title{\vspace{3cm}Common Crawl}
\subtitle{Analyse des Crawling-Prozesses und der Textextraktion}
\author{Erik Körner \and Kai Hainke \and Klemens Schölhorn}

% show toc on titlepage
\setuptoc{toc}{leveldown}

% no page number on titlepage
\renewcommand*{\titlepagestyle}{empty}

\begin{document}

\maketitle
\vspace{2cm}
\tableofcontents

\onehalfspacing

\chapter{Common Crawl}

\section{Organisation}

\begin{itemize}
    \item Non-Profit Organisation
    \item 2007 durch Gil Elbaz gegründet
    \item Mitglieder stellen unentgeldlich Fachwissen und Zeit zur Verfügung
\end{itemize}
\textbf{``Our goal is to democratize the data so everyone, not just big companies, can do high quality research and analysis.''}
\hfill-- \textit{Common Crawl Foundation}

\begin{itemize}
    \item (Fast) monatliche Crawls des Web und freie Bereitstellung der Datensätze
    \item Letzter Crawl: 168 TB, 2,1 Mrd. Websites\footnote{\url{http://blog.commoncrawl.org/2015/05/april-2015-crawl-archive-available/} (Stand: Mai 2015)}
    \item Viele Code-Beispiele zur Verarbeitung der Daten
    \item Hosting gesponsert von AWS (Public Data Set),\\gesamt: 541 TB\footnote{\url{https://aws.amazon.com/datasets/41740}}
\end{itemize}


\section{Technik}

\begin{itemize}
    \item CCBot: modifizierter Apache Nutch 1.7
        \begin{itemize}
            \item Basiert auf Lucene, Solr und Hadoop
            \item Verteiltes Crawling bei AWS
            \item „Freundlicher Crawler“: beachtet robots.txt und erzeugt so wenig Last wie möglich
        \end{itemize}
    \item Speicherung im WARC-Format (Web ARChive)
    \item Drei verschiedene Ergebnisse
        \begin{itemize}
            \item WARC: Enthält komplette HTTP Konversationen getrennt nach
                Anfrage und Antwort (Records)
            \item WAT: Metadaten für jeden Record
            \item WET: Reintext-Extrakt der Antworten
        \end{itemize}
\end{itemize}


\chapter{Analyse}

Die Analyse erfolgte anhand des Crawls vom April 2015, des zum Zeitpunkt des
Vortrages aktuellsten Crawls. Der komplette Crawl besitzt eine Größe von über
168 TB\footnote{\url{http://blog.commoncrawl.org/2015/05/april-2015-crawl-archive-available/}}.
Da eine Bearbeitung dieser Datenmenge auf eigenen Servern aufgrund von
Bandbreitenbeschränkungen und eine Bearbeitung in AWS mit Hilfe von EC2-Instanzen
aufgrund von finanziellen Einschränkungen nicht möglich war, haben wir unsere
Analyse auf die WAT-Dateien beschränkt, die aus knapp 39000 Dateien zu
durchschnittlich je 315 MiB bestehen und damit zusammen nur rund 12 TB groß sind.
Davon konnten wir bis zum Vortrag 53 \% analysieren.

\section{Übersicht}

\begin{wrapfigure}{r}{.54\textwidth}
\label{wrap-fig:1}
\begin{tikzpicture}[auto]
    \node[block] (CC) at (0,0) {Common Crawl};
    \node[block, right=3.5cm of CC] (META) {meta-extractor};
    \node[block, below=1.5cm of META] (MONET) {MonetDB};
    \node[block, left=3.5cm of MONET] (R) {R};
    \path[line] (CC) -- node {Download (curl)} (META);
    \path[line] (META) -- node {csv} (MONET);
    \path[line] (MONET) -- node {MonetDB.R} (R);
\end{tikzpicture}
\end{wrapfigure}

Die nebenstehende Abbildung zeigt den strukturellen Ablauf der Analyse.

Der Download und die Extraktion der benötigten Daten mit Hilfe des
\texttt{meta-extractor}s erfolgte parallel auf dem zur Verfügung gestellten
Praktikumsrechner und auf einem privaten Server eines Autors. Dabei wurden keine
Dateien lokal zwischengespeichert, sondern die WAT-Dateien mittels Unix-Pipes
direkt von \texttt{curl} in den \texttt{meta-extractor} und anschließend die
resultierenden csv-Dateien mittels \texttt{ssh} direkt auf den genannten
privaten Server zur weiteren Analyse geladen.

Dazu wurden die Daten in die spaltenorientierte Datenbank
\texttt{MonetDB}\footnote{\url{https://www.monetdb.org/}} importiert, um
anschließend mittels der \texttt{R}-Anbindung darauf in \texttt{R} Analysen
durchführen zu können.

\section{\texttt{meta-extractor}}

Der \texttt{meta-extractor} ist ein in \texttt{C++} geschriebenes Programm,
das sequentiell Metadaten-WARC-Records aus WAT-Dateien auslesen und daraus
die im Abschnitt \ref{csv-data} beschriebenen Metadaten als csv-Datei extrahieren
kann.

Dazu wurden neben einem csv-Writer ein standardkonformer WARC-Parser und eine
Trie-Implementierung für den effizienten Lookup des \texttt{Public Suffix} einer
Domain aus der \texttt{Public Suffix}-Liste implementiert.
Außerdem wurde auf RapidJSON\footnote{\url{https://github.com/miloyip/rapidjson}}
zum schnellen Parsen der WAT-Metadaten, auf die \texttt{URI}-Klasse des
POCO-Projektes\footnote{\url{http://pocoproject.org/}} und auf
TCLAP\footnote{\url{http://tclap.sourceforge.net/}} für das CLI zurückgegriffen.

Außerdem wurde das Test-Framework
Catch\footnote{\url{https://github.com/philsquared/Catch}} für Unit-Tests der
selbst geschriebenen Komponenten und für Integrationstests verwendet.

\section{Hilfsmittel}

Neben dem \texttt{meta-extractor} wurden noch einige Skripte u.\,a. zur
Steuerung des Downloads und der Extraktion, dem Import in \texttt{MonetDB} und
der Analyse in \texttt{R} geschrieben. Das erstgenannte ist etwas umfangreicher
und daher neben dem Hauptprogramm auch Teil des
Projektrepositories\footnote{\url{https://github.com/klemens/ALI-CC/}}.

\section{Gesammelte Metadaten}
\label{csv-data}

Die in folgender Tabelle beschriebenen Metadaten wurden vom \texttt{meta-extractor}
extrahiert und für die weiteren Analysen verwendet:

\begin{center}
\begin{tabular}{lcl}
    \textbf{Name}  & \textbf{Datentyp} & \textbf{Beispiel} \\ \hline\hline
    UUID           & uint128           & \textcolor{light-gray}{14a939f4-2355-11e5-b5f7-727283247c7f} \\ \hline
    Zeitstempel    & string            & \textcolor{light-gray}{2015-07-06T11:03:18T} \\ \hline
    Verwendung TLS & bool              & https ja/nein \\ \hline
    Hostname       & string            & \textcolor{light-gray}{amazon.co.uk} \\ \hline
    TLD            & string            & \textcolor{light-gray}{uk} \\ \hline
    Public Suffix  & string            & \textcolor{light-gray}{co.uk} \\ \hline
    Pfadtiefe      & uint8             & \textit{test/cool/cool.html} $\rightarrow$ \textcolor{light-gray}{3} \\ \hline
    Pfadlänge      & uint16            & \textit{test/cool/cool.html} $\rightarrow$ \textcolor{light-gray}{19} \\ \hline
    Server         & string            & \textcolor{light-gray}{apache}, \textcolor{light-gray}{nginx}, \dots \\ \hline
    Kompression    & bool              & \textcolor{light-gray}{gzip}, \textcolor{light-gray}{deflate} in \textit{Content-Encoding} \\ \hline
    Cookies        & bool              & \textit{Set-Cookie} vorhanden \\ \hline
    MIME-Typ       & string            & \textcolor{light-gray}{text/html}, \textcolor{light-gray}{application/xml}, \dots\\ \hline
    Charset        & string            & \textcolor{light-gray}{utf}, \textcolor{light-gray}{iso-8859}, \dots \\ \hline
    Verwendung CDN & bool              & CDN ja/nein \\ \hline
    Interne Links  & uint16            & Anzahl relative Links + gleicher Host \\ \hline
    Externe Links  & uint16            & Anzahl ausgehende Links
\end{tabular}
\end{center}



\chapter{Ergebnisse}


\section{TLD}
\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_tld_top10}}
Die TLD .com, die vor allem von kommerziellen Unternehmen genutzt wird, ist mit über 70\% mit Abstand am meisten vertreten. Danach folgen die eher von nicht-kommerziellen Organisationen benutzten TLD's .org, .edu, .net und .gov. Zusammen stellen Sie ungefähr 18\%. Dann folgen länderspezifische TLD's, wobei europäische und englischsprachige Länder den überwiegenden Anteil stellen (unter den Top 10 sind .de (Deutschland), .au (Australien), .ca (Kanada) und .fr (Frankreich)).
Grundsätzlich scheinen die TLD's zipfverteilt.


\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_tld}}

\vspace{.4cm}
\makebox[\textwidth][c]{\includegraphics[width=\textwidth]{images/selection_bias}}

Die vom Common Crawl erfassten Seiten stellen eine nicht randomisierte Stichprobe aus der Gesamtheit aller Websites dar. Daher unterliegen die Ergebnisse einer statistischen Verzerrung im Vergleich zur wahren Verteilung der Grundgesamtheit aller Websites, dem Selection Bias. Um abzuschätzen, in wie weit die Auswahl von Websites durch den Common Crawl Prozess im Vergleich zu einer randomisierten Auswahl verzerrt ist, vergleichen wir die Verteilung der TLD's mit Daten von Google. Auch diese Daten unterliegen natürlich dem Selection Bias, haben aber eine größere Stichprobengröße und deutliche Unterschiede in der Verteilung der TLD's können uns aufzeigen, welche TLD's vom Common Crawl eher erfasst bzw. nicht erfasst werden. So zeigt sich, dass asiatische TLD's unterrepräsentiert sind (bspw. Südkorea .kr). Auch sind typische TLD's von Websites für Medienstreaming unterrepräsentiert, wie .tv und .fm. Überrepräsentiert sind dafür .com, .edu und .net.

Dies ist ein Hinweis darauf, dass vor allem Websites kommerzieller Organisationen (.com) und offizieller Stellen (.edu, .gov) gecrawlt werden, während Websites mit vielem nicht-textualen Inhalt (.tv oder .fm) nicht gecrawlt werden.

\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_tld_comparison}}

\section{Public Suffix}
\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_pub_suffixes_top10}}

\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_pub_suffixes}}

Wie zu erwarten war, zeigt sich bei den Public Suffixes ein ähnliches Bild wie bei den TLD's. Es fällt jedoch der recht hohe Anteil an blogspot.com Seiten auf. Ebenfalls erscheinen zuerst co.uk und com.au in der Liste, welche hauptsächlich von kommerziellen Organisationen verwendet werden, was die Vermutung aus dem vorherigen Kapitel bestärkt.

\section{Mime-Type}
\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_mime}}

Es werden fast ausschließlich HTML/XML-Textdaten gecrawlt, was den Erwartungen entspricht.

\section{TLS}
\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_https}}
Der überwiegende Anteil an Websites, welche gecrawlt werden, sind unverschlüsselt.



\section{Encoding}
\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_charset}}

Das Encoding gibt erste Hinweise darauf, welche Zeichen auf den Websites verwendet werden. Die Mehrheit an gecrawlten Websites benutzt ein utf- oder iso-8859 Encoding. TODO

\section{Server}
\vspace{.1cm}
\makebox[\textwidth][c]{\includegraphics[width=1.1\textwidth]{plots/plot_server}}


\chapter{Textextraktion}

\centering{
    \includegraphics[trim=0 105px 0 0, clip=true, width=.9\textwidth]{images/guardian-website}
}

\scriptsize
\vspace{.1cm}
\url{http://www.theguardian.com/world/2013/jun/09/edward-snowden-nsa-whistleblower-surveillance}

\scriptsize
Edward Snowden: the whistleblower behind the NSA surveillance revelations $|$ US news $|$ The Guardian

\section{jWarcEx}

\begin{lstlisting}[breaklines=true]
Skip to main content

browse all sections close

Glenn Greenwald on security and liberty

Edward Snowden: the whistleblower behind the NSA surveillance revelations

The 29-year-old source behind the biggest intelligence leak in the NSA's history explains his motives, his uncertain future and why he never intended on hiding in the shadows

Q\&A with NSA whistleblower Edward Snowden: ``I do not expect to see home again''

Glenn Greenwald, Ewen MacAskill and Laura Poitras in Hong Kong

Tuesday 11 June 2013 09.00 EDT Last modified on Saturday 4 October 2014 10.54 EDT

Sorry, your browser is unable to play this video.

The individual responsible for one of the most significant leaks in US political history is Edward Snowden, a 29-year-old former technical assistant for the CIA and current employee of the defence contractor Booz Allen Hamilton. Snowden has been working at the National Security Agency for the last four years as an employee of various outside contractors, including Booz Allen and Dell.

The Guardian, after several days of interviews, is revealing his identity at his request. From the moment he decided to disclose numerous top-secret documents to the public, he was determined not to opt for the protection of anonymity. ``I have no intention of hiding who I am because I know I have done nothing wrong,'' he said.

(...)
\end{lstlisting}

\section{Mozilla Readability.js}

\begin{lstlisting}[breaklines=true]
Edward Snowden: the whistleblower behind the NSA surveillance revelations

Laura Poitras

The individual responsible for one of the most significant leaks in US political history is Edward Snowden, a 29-year-old former technical assistant for the CIA and current employee of the defence contractor Booz Allen Hamilton. Snowden has been working at the National Security Agency for the last four years as an employee of various outside contractors, including Booz Allen and Dell.

The Guardian, after several days of interviews, is revealing his identity at his request. From the moment he decided to disclose numerous top-secret documents to the public, he was determined not to opt for the protection of anonymity. ``I have no intention of hiding who I am because I know I have done nothing wrong,'' he said.

(...)
\end{lstlisting}

\section{Common Crawl}

\begin{lstlisting}[breaklines=true]
Lance Armstrong, Tour de France champ, retires | abc7.com

GO

Personalize your weather by entering a location.

Sorry, but the location you entered was not found. Please try again.

Sections

Traffic

Video

Los AngelesOrange CountyInland EmpireVentura CountyCalifornia

Home

Accuweather

Traffic

Video

Photos

Mobile Apps

Local News Los AngelesOrange CountyInland EmpireVentura CountyCalifornia

Map My News

Shows Live Well Network

Follow Us

BREAKING NEWS

(...)
\end{lstlisting}

\vspace{.2cm}
\url{http://abc7.com/archive/7962461}

\section{jWarcEx}

\begin{lstlisting}[breaklines=true]
Personalize your weather by entering a location.

Sorry, but the location you entered was not found. Please try again.

\vspace{.2cm}
Sections Traffic Video Los AngelesOrange CountyInland EmpireVentura CountyCalifornia

Home Accuweather Traffic Video Photos Mobile Apps

Los AngelesOrange CountyInland EmpireVentura CountyCalifornia

\quad BREAKING NEWS ABC shows live and on-demand -- Download the WATCH ABC app!

\vspace{.2cm}
Lance Armstrong, Tour de France champ, retires

Seven-time Tour de France champion Lance Armstrong is retiring. He ends his career amidst a federal doping investigation.

February 16, 2011 12:00:00 AM PST

Seven-time Tour de France champion Lance Armstrong said he's retiring from the professional cycling circuit, this time for good. The 39-year-old is calling it ``Retirement 2.0'' and is making it clear there is no reset button this time.

\vspace{.2cm}
He says he wants to spend more time with his children and dedicate himself even more to the fight against cancer with his "Livestrong" Foundation.

(...)
\end{lstlisting}

\vspace{.2cm}
\url{http://abc7.com/archive/7962461}


\end{document}
